{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import json\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "import wikipedia\n",
    "import wptools\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# For each category we only select one type of dbpedia object to be sure what we have is what we need\n",
    "catType = {\"Airports\": \"?a a dbo:Infrastructure .\", \"Comics_characters\": \"?a a dbo:Agent .\",\n",
    "           \"Artist\": \"?a a dbo:Animal .\",\n",
    "           \"Astronauts\": '?a a dbo:Animal .', \"Building\": \"?a a dbo:Building .\", \"Astronomical_objects\": \"\",\n",
    "           \"City\": \"?a a dbo:Place .\", \"Companies\": \"?a a dbo:Company .\", \"Foods\": \"?a a dbo:Food .\",\n",
    "           \"Transport\": \"?a a dbo:MeanOfTransportation .\", \"Monuments_and_memorials\": \"?a a dbo:Place .\",\n",
    "           \"Politicians\": \"?a a dbo:Animal .\", \"Sports_teams\": \"?a a dbo:Organisation .\",\n",
    "           \"Sportspeople\": \"?a a dbo:Animal .\", \"Universities_and_colleges\": \"?a a dbo:Organisation .\",\n",
    "           \"Written_communication\": \"\"}\n",
    "\n",
    "# All the object we need\n",
    "stop_words = stopwords.words('english')\n",
    "porter = PorterStemmer()\n",
    "sparql_dbpedia = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
    "sparql_dbpedia.setReturnFormat(JSON)\n",
    "sparql_wd = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
    "sparql_wd.setReturnFormat(JSON)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "k = 15 # the number of item per category\n",
    "n = 2 # The number of line we need per wikipedia page to keep it"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 1 :"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = [] # We init the list of the articles\n",
    "for cat in catType:\n",
    "\n",
    "    # Sparql request for getting info we need to recover the pages we need per category\n",
    "    # We make sure to remove the list of something articles\n",
    "\n",
    "    SPARQL_GET_LISTS = f'''\n",
    "    PREFIX dbr: <http://dbpedia.org/resource/>\n",
    "    PREFIX dbp: <http://dbpedia.org/property/>\n",
    "    PREFIX dbc: <http://dbpedia.org/resource/Category:>\n",
    "    PREFIX skos: <http://www.w3.org/2004/02/skos/core#>\n",
    "    PREFIX dct: <http://purl.org/dc/terms/>\n",
    "\n",
    "    SELECT DISTINCT ?page ?p ?code WHERE {{\n",
    "    ?a dct:subject/skos:broader dbc:{cat} .\n",
    "    {catType[cat]}\n",
    "    ?a rdfs:label ?label .\n",
    "    ?a foaf:name ?page .\n",
    "    ?p foaf:primaryTopic ?a .\n",
    "    ?a dbo:wikiPageID ?code .\n",
    "    FILTER (lang(?label) = 'en')\n",
    "    FILTER(!STRSTARTS(?label, \"List of\"))\n",
    "    }}limit {k}\n",
    "    '''\n",
    "    sparql_dbpedia.setQuery(SPARQL_GET_LISTS)\n",
    "    try:\n",
    "        ret = sparql_dbpedia.queryAndConvert()\n",
    "        for r in ret[\"results\"][\"bindings\"]: # for each result\n",
    "            url_name = r[\"code\"]['value']  # get the wikipedia page id\n",
    "            page = wikipedia.page(pageid=url_name) # get the page\n",
    "            cont = page.content\n",
    "            number_of_sentences = len(sent_tokenize(cont)) # count the number of sentence we have with nltk\n",
    "            if number_of_sentences >= n:\n",
    "                p = wptools.page(pageid=url_name, silent=True)\n",
    "                p.get_parse() # get infobox\n",
    "                p.get_wikidata() #get the wikidata info\n",
    "                data.append( # We store the data\n",
    "                    {\"name\": r[\"page\"]['value'], \"url_name\": url_name, \"txt\": page.content,\n",
    "                     \"infobox\": p.data[\"infobox\"],\n",
    "                     \"wikidata\": p.data[\"wikidata\"],\n",
    "                     \"cat\": cat, \"url\": r[\"p\"]['value'], \"wikibase\": p.data[\"wikibase\"]})\n",
    "    except Exception as e:\n",
    "        print()\n",
    "# We save it to be sure this costly method doesn't need to be rerun\n",
    "with open(\"data.json\", \"w\") as f:\n",
    "    json.dump(data, f)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 2 :\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "file = open(\"data.json\", \"r\") # We load the data from the file\n",
    "data_to_process = json.load(file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# this method is used to pre-process text\n",
    "# We make it lower,than remove the punctuation, than tokenize, we remove stop words\n",
    "# We stem every word (get the base word without conjugation\n",
    "# We tag every word, used later to compare data\n",
    "def preprocess(txt):\n",
    "    if type(txt) == str:\n",
    "        text = txt.lower()\n",
    "        text_p = \"\".join([char for char in text if char not in string.punctuation])\n",
    "        words = word_tokenize(text_p)\n",
    "        filtered_words = [word for word in words if word not in stop_words]\n",
    "        stemmed = [porter.stem(word) for word in filtered_words]\n",
    "        pos = pos_tag(stemmed)\n",
    "        return pos, stemmed\n",
    "    else:\n",
    "        return txt\n",
    "\n",
    "# This method will get the description in english of every wikidata item we have in the copus\n",
    "def getDescription(items):\n",
    "    item = \" wd:\".join(items) # We generate a list of item in sparql\n",
    "    SPARQL_DESCR = f'''\n",
    "    PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "    PREFIX schema: <http://schema.org/>\n",
    "    SELECT distinct ?o ?item WHERE {{\n",
    "    ?item schema:description ?o .\n",
    "    FILTER ( lang(?o) = \"en\" )\n",
    "    VALUES ?item {{ wd:{item} }}\n",
    "    }}\n",
    "    '''\n",
    "    sparql_wd.setQuery(SPARQL_DESCR)\n",
    "    try:\n",
    "        res = sparql_wd.queryAndConvert()\n",
    "        retu = {}\n",
    "        for x in res[\"results\"][\"bindings\"]:\n",
    "            retu.update({x['item']['value'].replace(\"http://www.wikidata.org/entity/\", \"\"): x['o']['value']}) #get in the dict the item and update the description\n",
    "        return retu\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "res = []\n",
    "to_get = []\n",
    "for item in data_to_process:\n",
    "    txt = preprocess(item[\"txt\"]) # Preprocess the text\n",
    "    to_get.append(item[\"wikibase\"]) # Add item to get the description later\n",
    "    res.append(\n",
    "        {\"person\": item['name'], \"text\": item['txt'], \"processed_text\": txt, \"desc\": \"\", \"processed_desc\": \"\",\n",
    "         \"cat\": item['cat'], \"base\": item[\"wikibase\"]})\n",
    "\n",
    "descr = getDescription(to_get) # Get the description of every item\n",
    "for x in res:\n",
    "    x.update({\"desc\": descr.get(x['base']), \"processed_desc\": preprocess(descr.get(x['base']))}) #process the description and save evrything\n",
    "\n",
    "df = pd.DataFrame(res) # Create the dataframe needed in the project"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# We save the dataframe to recover it later\n",
    "with open(\"dataframe.json\",\"w\") as f :\n",
    "    f.write(df.to_json())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}